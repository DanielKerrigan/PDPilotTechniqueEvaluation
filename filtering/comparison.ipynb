{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate_shapes import (\n",
    "    read_json,\n",
    "    get_scores,\n",
    "    plot_accuracy_vs_threshold,\n",
    "    check_labels,\n",
    "    plot_disagreements,\n",
    "    plot_label_counts,\n",
    "    fix_labels,\n",
    "    calculate_accuracy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curves_a = read_json(\"scratch/dan-debug-labeled-pdps.json\")\n",
    "df_a = get_scores(curves_a)\n",
    "labels_a = [x[\"shape\"] for x in curves_a]\n",
    "plot_a, best_a = plot_accuracy_vs_threshold(df_a)\n",
    "bad_labels_a = check_labels(\n",
    "    curves_a, df_a[df_a[\"threshold\"] == 0][\"labels\"].to_numpy()[0]\n",
    ")\n",
    "bad_labels_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curves_b = read_json(\"scratch/enrico-debug-labeled-pdps.json\")\n",
    "df_b = get_scores(curves_b)\n",
    "labels_b = [x[\"shape\"] for x in curves_b]\n",
    "plot_b, best_b = plot_accuracy_vs_threshold(df_b)\n",
    "bad_labels_b = check_labels(\n",
    "    curves_b, df_b[df_b[\"threshold\"] == 0][\"labels\"].to_numpy()[0]\n",
    ")\n",
    "bad_labels_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_disagreements(\n",
    "    [curves_b[x[\"index\"]] for x in bad_labels_b],\n",
    "    [x[\"user_label\"] for x in bad_labels_b],\n",
    "    [x[\"heuristic_label\"] for x in bad_labels_b],\n",
    "    \"User\",\n",
    "    \"Heuristic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_labels(curves_b, bad_labels_b[1:])\n",
    "labels_b = [x[\"shape\"] for x in curves_b]\n",
    "plot_b, best_b = plot_accuracy_vs_threshold(df_b)\n",
    "bad_labels_b = check_labels(\n",
    "    curves_b, df_b[df_b[\"threshold\"] == 0][\"labels\"].to_numpy()[0]\n",
    ")\n",
    "bad_labels_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_label_counts(labels_a, labels_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_disagreements(curves_a, labels_a, labels_b, \"User A\", \"User B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_accuracy(labels_a, labels_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdpilot-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
