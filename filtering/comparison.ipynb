{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate_shapes import (\n",
    "    read_json,\n",
    "    add_indices,\n",
    "    get_scores,\n",
    "    plot_accuracy_vs_threshold,\n",
    "    check_labels,\n",
    "    plot_disagreements,\n",
    "    plot_label_counts,\n",
    "    fix_labels,\n",
    "    set_consensus_labels,\n",
    "    calculate_num_correct,\n",
    "    get_best_thresholds,\n",
    "    check_same_curves,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load unlabeled curves\n",
    "curves_original = read_json(\"small-pdps.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read labeled PDPs from JSON file\n",
    "curves_a = add_indices(read_json(\"dan-small-labeled-pdps.json\"))\n",
    "print(f\"{len(curves_a)} curves\")\n",
    "\n",
    "# check that the labeled curves match the unlabeled\n",
    "check_same_curves(curves_a, curves_original)\n",
    "\n",
    "# get the user's shape labels as a list\n",
    "labels_a = [x[\"shape\"] for x in curves_a]\n",
    "\n",
    "# for each threshold, get the heuristic's labels and calculate\n",
    "# the accuracy of the user's labels wrt the heuristic's\n",
    "df_a = get_scores(curves_a)\n",
    "display(get_best_thresholds(df_a))\n",
    "\n",
    "# get a line chart that shows the accuracy of the\n",
    "# user's labels vs. the threshold\n",
    "plot_a = plot_accuracy_vs_threshold(df_a)\n",
    "display(plot_a)\n",
    "\n",
    "# check for mistakes in labels\n",
    "bad_labels_a = check_labels(\n",
    "    curves_a, df_a[df_a[\"threshold\"] == 0][\"labels\"].to_numpy()[0]\n",
    ")\n",
    "print(\"possible mistakes:\")\n",
    "print(bad_labels_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read labeled PDPs from JSON file\n",
    "curves_b = add_indices(read_json(\"enrico-small-labeled-pdps.json\"))\n",
    "print(f\"{len(curves_b)} curves\")\n",
    "\n",
    "# check that the labeled curves match the unlabeled\n",
    "check_same_curves(curves_b, curves_original)\n",
    "\n",
    "# get the user's shape labels as a list\n",
    "labels_b = [x[\"shape\"] for x in curves_b]\n",
    "\n",
    "# for each threshold, get the heuristic's labels and calculate\n",
    "# the accuracy of the user's labels wrt the heuristic's\n",
    "df_b = get_scores(curves_b)\n",
    "display(get_best_thresholds(df_b))\n",
    "\n",
    "# get a line chart that shows the accuracy of the\n",
    "# user's labels vs. the threshold\n",
    "plot_b = plot_accuracy_vs_threshold(df_b)\n",
    "display(plot_b)\n",
    "\n",
    "# check for mistakes in labels\n",
    "bad_labels_b = check_labels(\n",
    "    curves_b, df_b[df_b[\"threshold\"] == 0][\"labels\"].to_numpy()[0]\n",
    ")\n",
    "print(\"possible mistakes:\")\n",
    "bad_labels_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_num_correct(labels_a, labels_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the identified mistakes\n",
    "plot_disagreements(\n",
    "    [curves_b[x[\"index\"]] for x in bad_labels_b],\n",
    "    [x[\"user_label\"] for x in bad_labels_b],\n",
    "    [x[\"heuristic_label\"] for x in bad_labels_b],\n",
    "    \"User\",\n",
    "    \"Heuristic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first case does not look like a mistake, but the rest do\n",
    "fix_labels(curves_b, bad_labels_b[1:])\n",
    "\n",
    "# update based on corrected labels\n",
    "\n",
    "labels_b = [x[\"shape\"] for x in curves_b]\n",
    "\n",
    "df_b = get_scores(curves_b)\n",
    "display(get_best_thresholds(df_b))\n",
    "\n",
    "plot_b = plot_accuracy_vs_threshold(df_b)\n",
    "display(plot_b)\n",
    "\n",
    "# check that only the first case from above is still identified\n",
    "bad_labels_b = check_labels(\n",
    "    curves_b, df_b[df_b[\"threshold\"] == 0][\"labels\"].to_numpy()[0]\n",
    ")\n",
    "print(\"possible mistakes:\")\n",
    "bad_labels_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the same PDPs were labeled\n",
    "check_same_curves(curves_a, curves_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_label_counts(labels_a, labels_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_disagreements(curves_a, labels_a, labels_b, \"User A\", \"User B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_num_correct(labels_a, labels_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_num_correct(labels_a, labels_b) / len(curves_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consensus labels:\n",
    "\n",
    "- 3 mixed\n",
    "- 23 mixed\n",
    "- 33 decreasing\n",
    "- 36 decreasing\n",
    "- 48 mixed\n",
    "- 49 mixed\n",
    "- 52 mixed\n",
    "- 56 increasing\n",
    "- 65 increasing\n",
    "- 71 decreasing\n",
    "- 79 mixed\n",
    "- 87 mixed\n",
    "- 92 increasing\n",
    "- 102 mixed\n",
    "- 111 increasing\n",
    "- 129 increasing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrections = [\n",
    "    (3, \"mixed\"),\n",
    "    (23, \"mixed\"),\n",
    "    (33, \"decreasing\"),\n",
    "    (36, \"decreasing\"),\n",
    "    (48, \"mixed\"),\n",
    "    (49, \"mixed\"),\n",
    "    (52, \"mixed\"),\n",
    "    (56, \"increasing\"),\n",
    "    (65, \"increasing\"),\n",
    "    (71, \"decreasing\"),\n",
    "    (79, \"mixed\"),\n",
    "    (87, \"mixed\"),\n",
    "    (92, \"increasing\"),\n",
    "    (102, \"mixed\"),\n",
    "    (111, \"increasing\"),\n",
    "    (129, \"increasing\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from JSON file to get a copy that we will modify\n",
    "curves_consensus = add_indices(read_json(\"dan-small-labeled-pdps.json\"))\n",
    "# set the consensus labels for the disagreements\n",
    "set_consensus_labels(curves_consensus, labels_a, labels_b, corrections)\n",
    "\n",
    "# get the consensus shape labels as a list\n",
    "labels_consensus = [x[\"shape\"] for x in curves_consensus]\n",
    "\n",
    "# for each threshold, get the heuristic's labels and calculate\n",
    "# the accuracy of the consensus labels wrt the heuristic's\n",
    "df_consensus = get_scores(curves_consensus)\n",
    "best_thresholds_consensus = get_best_thresholds(df_consensus)\n",
    "display(best_thresholds_consensus)\n",
    "\n",
    "# get a line chart that shows the accuracy of the\n",
    "# consensus labels vs. the threshold\n",
    "plot_consensus = plot_accuracy_vs_threshold(df_consensus)\n",
    "display(plot_consensus)\n",
    "\n",
    "# check for mistakes in labels\n",
    "bad_labels_consensus = check_labels(\n",
    "    curves_consensus,\n",
    "    df_consensus[df_consensus[\"threshold\"] == 0][\"labels\"].to_numpy()[0],\n",
    ")\n",
    "print(\"possible mistakes:\")\n",
    "print(bad_labels_consensus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_consensus).mark_line().encode(\n",
    "    x=alt.X(\"threshold\").title(\"PDP shape labeling function tolerance parameter (t)\"),\n",
    "    y=alt.Y(\"accuracy\").title(\"Agreement with authors' labels\").axis(format=\".2~%\"),\n",
    ").properties(width=400, height=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that for all of the best thresholds, the heurisitc labels are the same\n",
    "\n",
    "heuristic_labels = best_thresholds_consensus[\"labels\"].to_numpy()[0]\n",
    "\n",
    "for labels in best_thresholds_consensus[\"labels\"].to_numpy():\n",
    "    assert heuristic_labels == labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_disagreements(\n",
    "    curves_a, labels_consensus, heuristic_labels, \"Consensus\", \"Heuristic\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdpilot-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
